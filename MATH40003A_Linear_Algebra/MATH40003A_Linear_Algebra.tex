\documentclass[../Year1/Year1.tex]{subfiles}
\usepackage{import}
\usepackage{../style/header}
\def\module{MATH40003A Linear Algebra}
\def\lecturer{Dr Charlotte Kestner}
\def\term{Autumn \& Spring 2023}
\def\cover{\vspace{1in}
\[
\begin{tikzcd}[ampersand replacement=\&, column sep=tiny]
\& \qquad \& \& \& L \arrow[dash, dashed]{dddd} \& \& \& \& \& \& \\
K\br{\alpha} \arrow[dash]{urrrr} \arrow[dash, dashed]{dddd} \& \& K\br{\alpha'} \arrow[dash]{urr} \arrow[dash, dashed]{dddd} \& \& \& K\br{\beta, \gamma} \arrow[dash]{ul} \arrow[dash, dashed]{dddd} \& \& \& K\br{\delta} \arrow[dash]{ullll} \arrow[dash, dashed]{dddd} \& \& K\br{\delta'} \arrow[dash]{ullllll} \arrow[dash, dashed]{dddd} \\
\& \& \& K\br{\beta} \arrow[crossing over, dash]{ulll} \arrow[dash]{ul} \arrow[crossing over, dash]{urr} \arrow[dash, dashed]{dddd} \& \& \& K\br{\beta\gamma} \arrow[crossing over, dash]{ul} \arrow[dash, dashed]{dddd} \& \& \& K\br{\gamma} \arrow[crossing over, dash]{ullll} \arrow[crossing over, dash]{ul} \arrow[crossing over, dash]{ur} \arrow[dash, dashed]{dddd} \& \\
\& \& \& \& \& \& \& K \arrow[crossing over, dash]{ullll} \arrow[crossing over, dash]{ul} \arrow[crossing over, dash]{urr} \arrow[dash, dashed]{dddd} \& \& \& \\
\& \qquad \& \& \& \abr{e} \& \& \& \& \& \& \\
\abr{\tau} \arrow[dash]{urrrr} \& \& \abr{\sigma^2\tau} \arrow[dash]{urr} \& \& \& \abr{\sigma^2} \arrow[dash]{ul} \& \& \& \abr{\sigma\tau} \arrow[dash]{ullll} \& \& \abr{\sigma^3\tau} \arrow[dash]{ullllll} \\
\& \& \& \abr{\sigma^2, \tau} \arrow[dash]{ulll} \arrow[dash]{ul} \arrow[dash]{urr} \& \& \& \abr{\sigma} \arrow[dash]{ul} \& \& \& \abr{\sigma^2, \sigma\tau} \arrow[dash]{ullll} \arrow[dash]{ul} \arrow[dash]{ur} \& \\
\& \& \& \& \& \& \& G \arrow[dash]{ullll} \arrow[dash]{ul} \arrow[dash]{urr} \& \& \&
\end{tikzcd}
\]
\[ G = \Gal\br{L / K} \cong \DDD_8 \]
}
\def\syllabus{Systems of linear equations, Matrices, Augmented matrices, Elementary matrices, EROs, REF \& rREF, Linear maps, Fields, Vector Spaces, Subspaces, Spanning, Linear independence, Bases, Rank Nullity, Representations of bases, Determinants, Eigenvalues and eigenvectors, characteristic polynomial, diagonisability, orthogonality, Gramm-Schmidt process, symmetric matrices, spectral theorem}
\def\thm{subsection}

\begin{document}

%\input{../style/cover}


\section{Introduction}

\lecture{1}{Thursday}{10/01/19}

The following are references.
\begin{itemize}
\item E Artin, Galois theory, 1994
\item A Grothendieck and M Raynaud, Rev\^etements \'etales et groupe fondamental, 2002
\item I N Herstein, Topics in algebra, 1975
\item M Reid, Galois theory, 2014
\end{itemize}

\begin{notation*}
If $ K $ is a field, or a ring, I denote
the \textbf{ring of polynomials} with coefficients in $ K $.
\end{notation*}

\section{Linear Systems and matrices}
\subsection{Linear systems}
\begin{definition}[Linear system]
    A \textbf{linear system} is a set of linear equations in the same variables.
\end{definition}

\begin{notation}
    The follow are all equivalent notation for the same linear system:
    \[
        \begin{matrix}
        a_{11}x_1 & + & a_{12}x_2 & + & \dots & + & a_{1n}x_n & = & b_1\\
        a_{21}x_1 & + & a_{22}x_2 & + & \dots & + & a_{2n}x_n & = & b_2\\
        \vdots & & \vdots & & \ddots & & \vdots & & \vdots\\
        a_{m1}x_1 & + & a_{m2}x_2 & + & \dots & + & a_{mn}x_n & = & b_m\\
        \end{matrix}
        \iff
        \begin{pmatrix}
        a_{11} & a_{12} & \dots & a_{1n}\\
        a_{21} & a_{22} & \dots & a_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{m1} & a_{m2} & \dots & a_{mn}\\
        \end{pmatrix}
        \begin{pmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n \\
        \end{pmatrix} = 
        \begin{pmatrix}
            b_1 \\
            b_2 \\
            \vdots \\
            b_m \\
        \end{pmatrix}
    \]
    \[
        \iff
        \begin{amatrix}{4}
        a_{11} & a_{12} & \dots & a_{1n} & b_{1}\\
        a_{21} & a_{22} & \dots & a_{2n} & b_{2}\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        a_{m1} & a_{m2} & \dots & a_{mn} & b_{m}\\
        \end{amatrix}
        \textcolor{black}{.}
    \]
\end{notation}

\vspace{-30pt}

\subsection{Matrix algebra}
\begin{definition}[Matrix by elements]
    An $m\times n$ matrix written as $A = [a_{ij}]_{m\times n}$ has the element $a_{ij}$ in the $i$th row and $j$th column.
\end{definition}

\begin{definition}[Matrix addition]
    If $A = [a_{ij}]_{m\times n}$ and $B = [b_{ij}]_{m\times n}$ then $A+B := [a_{ij} + b_{ij}]_{m\times n}$.
\end{definition}

\begin{definition}[Scalar multiplication]
    If $A = [a_{ij}]_{m\times n}$ then $\lambda A := [\lambda a_{ij}]_{m\times n}$.
\end{definition}

\begin{definition}[Matrix multiplication]
    If $A = [a_{ij}]_{p\times q}$ and $B = [b_{ij}]_{q\times r}$ then $AB := C = [c_{ij}]_{p\times r}$ where $c_{ij} = \sum\limits_{k=1}^qa_{ik}b_{kj}$.
\end{definition}

\begin{theorem}
    Matrix multiplication is associative.
\end{theorem}

\begin{remark}
    Matrix multiplication is not commutative.
\end{remark}

\subsection{EROs}
\begin{definition}[Elementary row operations]
    The three \textbf{elementary row operations (EROs)} that can be performed on augmented matrixes are as follows:
    \begin{enumerate}
        \item Multiply a row by a non-zero scalar.
        \item Swap two rows.
        \item Add a scalar multiple of a row to another row.
    \end{enumerate}
\end{definition}

\begin{remark}
    EROs preserve the set of solutions of a linear system. Each ERO has an inverse.
\end{remark}

\begin{definition}[Equivalence of linear systems]
    Two systems of linear equations are equivalent iff either:
    \begin{enumerate}
        \item They are both inconsistent.
        \item (wlog) The augmented matrix of the first system can be transformed to the augmented matrix of the second system with just EROs.
    \end{enumerate}
\end{definition}

\begin{definition}[Row echelon form / Echelon form/ REF]
    A matrix is in \textbf{row echelon form} if it satisifes the following: \begin{enumerate}
        \item All of the zero rows are at the bottom of the matrix,
        \item The first non-zero entry in any row is $1$,
        \item The first non-zer entru in row is stricly to the left of the first non-zero entry in row $i+1$.
    \end{enumerate}
\end{definition}

\begin{definition}[Reduced row echelon form / Row reduced echelon form / rREF]
    A matrix is im \textbf{reduced row echelon form} if it is in REF and the first non-zero entry in a row is the only non-zero entry in its column.
\end{definition}

\subsection{Matirces of note}

\begin{definition}[Square matirx]
    A matrix is \textbf{square} iff it has the same number of rows and columns.
\end{definition}

\begin{definition}
    A square matrix ($A = [a_{ij}]_{n\times n}$) is: \begin{enumerate*}
        \item \textbf{Upper triangular} iff $i>j \implies a_{ij}=0$.
        \item \textbf{Lower triangular} iff $i<j \implies a_{ij}=0$.
        \item \textbf{Diagonal} iff $i\neq j \implies a_{ij}=0$ .
    \end{enumerate*}
\end{definition}

\begin{definition}[Identity matrix]
    The \textbf{identity matrix} of size $n$ written $\I_{n}$, is the square diagonal matrix of size $n$ with all diagonal entries equal $1$.
\end{definition}

\begin{definition}[Elementary matrix]
    An \textbf{elementary matrix} is a matrix that can be achieved by appling a single ERO to the identity matrix.
\end{definition}

\begin{definition}[Inverse]
    For a square matrix $B$ if there exists a matrix $B^{-1}$ such that $\textbf{BB}^{-1} = I = B^{-1}B$ then $B^{-1}$ is the \textbf{inverse} of $B$ and vice versa.
\end{definition}

\begin{definition}[Singular]
    A matrix without an inverse is \textbf{singular}.
\end{definition}

\begin{theorem}
    The inverse of a matrix is unique.
\end{theorem}

\begin{definition}
    A \textbf{transpose} of the matrix $A = [a_{ij}]_{m\times n}$ is  $A^\T := [a_{ji}]_{n\times m}$.
\end{definition}

\begin{theorem}
    If the matrix $A$ has an inverse then its transpose has an inverse with $(A^\T)^{-1}=(A^{-1})^\T$.
\end{theorem}

\begin{theorem}
    If a matrix $A\in M_{m\times n}$ can be reduced to $\I_n$ by a sequence of EROs then $A$ is inevitable with $A^{-1}$ given by applying the same sequence of EROs to $\I_n$.
\end{theorem}

\begin{definition}
    A matrix $A$ is \textbf{orthogonal} if it has an inverse with $A^{-1}=A^\T$.
\end{definition}

\begin{theorem}
    An orthogonal matrix $A$ satisfies the condition $(Ax) \cdot (Ay) = x \cdot y$, where $\cdot$ is the dot product.
\end{theorem}

\section{Vector Spaces}
The notion of a vector space is a structure designed to generalise that of real vectors, so before developing them we must first produce a generalisation of the real numbers.
\subsection{Fields}
\begin{definition}[Field]
    A \textbf{field} is a set $\FF$ equipped with the binary operations \textbf{addition} $+:\FF\times\FF\rightarrow\FF$ and \textbf{multiplication} $\cdot:\FF\times\FF\rightarrow\FF$ satisfying the follow axioms:
    \begin{enumerate}
        \item[A1] $\forall x,y \in \FF: x+y=y+x$ (commutativity of addition),
        \item[A2] $\forall x,y,z \in \FF: x+(y+z)=(x+y)+z$ (associativity of addition),
        \item[A3] $\exists 0_\FF \in \FF$ such that $\forall x\in\FF: x + 0_\FF = x$, (additive identity element),
        \item[A4] $\forall x\in\FF, \ \exists (-x)\in\FF$ such that $x+(-x)=0_\FF$, (additive inverse);
        
        \item[M1] $\forall x,y \in \FF: x\cdot y=y\cdot x$ (commutativity of multiplication),
        \item[M2] $\forall x,y,z \in \FF: x\cdot (y\cdot z)=(x\cdot y)\cdot z$ (associativity of multiplication),
        \item[M3] $\exists 1_\FF \in \FF$ such that $\forall x\in\FF: x \cdot  1_\FF = x$, (multiplicative identity element),
        \item[M4] $\forall x\in\FF\setminus\{0_\FF\}, \ \exists x^{-1}\in\FF$ such that $x\cdot x^{-1}=1_\FF$, (multiplicative inverse);

        \item[D] $\forall x,y,z \in \FF: x\cdot(y+z)=x\cdot y+x\cdot z$ (distributivity of multiplication over addition).
    \end{enumerate}
    The field $(\FF,+,\cdot)$ is often referred to as just $\FF$.
\end{definition}

\begin{example}
    The familiar sets $\RR, \QQ, \CC$ are all fields.
\end{example}

\begin{theorem}
    If $p\in\NN$ is prime with $\FF_p = \{0,1,\ldots,p-1\}$ then $(\FF_p,+_p,\cdot_p)$ is a field.
\end{theorem}

\subsection{Vector spaces}
\begin{definition}[Vector space]
    A \textbf{vector space} over a field $\FF$ is a set $V$ equipped with the binary operations \textbf{vector addition} $\oplus:V\times V\rightarrow V$ and \textbf{scalar multiplication} $\odot:\FF\times V\rightarrow V$ satisfying the follow axioms:
    \begin{enumerate}
        \item[A1] $\forall u,v,w \in V: u\oplus(v\oplus w)=(u\oplus v)\oplus w$ (associativity of addition),
        \item[A2] $\forall u,v \in V: u\oplus v=v\oplus u$ (commutativity of vector addition),
        \item[A3] $\exists 0_V \in V$ such that $\forall v\in V: v + 0_V = v$, (vector additive identity element),
        \item[A4] $\forall v\in V, \ \exists (-v)\in v$ such that $v+(-v)=0_V$, (vector additive inverse),

        \item[A5] $\forall x \in \FF, \ \forall u,v\in V: x \odot (u\oplus v) =(x\odot u) \oplus (x\odot v)$ (vector distributivity 1),
        \item[A6] $\forall x,y \in \FF, \ \forall v\in V: x\cdot (x+y)\odot v =(x\odot v) \oplus (y\odot v)$ (vector distributivity 2),
        \item[A7] $\forall x,y \in \FF, \ \forall v\in V: (x\cdot y)\odot v = x\odot (y\odot v)$ (scalar multiplication associativity),
        \item[A8] $\forall v\in V: 1_F\odot v = v$, (scalar multiplication identity element).
    \end{enumerate}
    If $V$ is a vector space over $\FF$ we say $V$ is an $\FF$-vector space with $v\in V$ a \textbf{vector} and $x\in\FF$ a \textbf{scalar}.
\end{definition}

\subsection{Subspaces}
\begin{definition}[Subspace]
    A subset $W\subseteq V$ is a \textbf{subspace} of $V$, denoted $W\leq V$ iff:
    \begin{enumerate}
        \item[S1] $W\neq \emptyset$,
        \item[S2] $\forall w_1, w_2 \in W: w_1 \oplus w_2\in W$,
        \item[S3] $\forall x\in\FF, \  \forall w\in W: x\odot w\in W$.
    \end{enumerate}
    If $W=\{0_V\}$ then $W$ is the \textbf{trivial subspace}.
\end{definition}

\begin{theorem}
    Every subspace of V contains $0_V$.
\end{theorem}

\begin{theorem}
    If $U$ and $W$ are subspaces of $V$, $U\cap W$ is a subspace of $V$.
\end{theorem}

\section{Spanning and Linear Independence}
Throughout this section, assume $V$ is an $\FF$-vector space.
\subsection{Spanning}
\begin{definition}[Span]
    Given some set $\{v_1,v_2,\ldots,v_n\}\subseteq V$ define the \textbf{span} by, \[
    \text{Span}(\{v_1,v_2,\ldots,v_n\}):=\{u\in V: u = \sum_{i=1}^n\alpha_i v_i \ \text{\textcolor{black}{ with }}\alpha_i\in\FF\}\textcolor{black}{.}
    \]
    Note that the span of a subset of $V$ is always a subspace of $V$.
\end{definition}

\begin{remark}
    If $S\subseteq V$ is infinite, $\text{Span}(S)$ is the set of all \textbf{finite} linear combinations of elements of $S$.
\end{remark}

\begin{definition}[Spanning sets]
    If $S\subseteq V$ and $\text{Span}(S)=V$, we say $S$ \textbf{spans} $V$ or $S$ is a \textbf{spanning set} for $V$.
\end{definition}

\subsection{Linear independence}
\begin{definition}
    The set $\{v_1,v_2,\ldots,v_n\}\subseteq V$ is \textbf{linearly independent} in $V$ iff: \[
        \sum_{i=1}^n\alpha_i v_i = 0_V \iff \alpha_i=0_\FF \ \text{\textcolor{black}{ for all }}\i\in[1,n]\textcolor{black}{.}
    \]
\end{definition}

\begin{theorem}
    If $S = \{v_1,v_2,\ldots,v_n\}\subseteq V$ is linearly independent in $V$ with $v_{n+1}\in V\setminus \text{Span}(S)$ then $S\cup\{v_{n+1}\}$ is also linearly independent in $V$. 
\end{theorem}

\section{Bases}

\subsection{Definition}
Again, assume $V$ is an $\FF$-vector space throughout this section.
\begin{definition}[Bases]
    A \textbf{basis} for $V$ is linearly independent, spanning set of $V$. If $V$ has a finite bases then $V$ is said to be a \textbf{finite dimensional} vector space.
\end{definition}

\begin{theorem}
    Any $S\subseteq V$ is a basis for $V$ iff every vector in $V$ can be uniquely expressed as a linear combination of the elements of $S$.
\end{theorem}

\begin{theorem}
    If $V$ is non-trivial and $S$ is a finite spanning set of $V$ then $S$ contains a basis for $V$.
\end{theorem}

\begin{lemma}[Steinitz Echange Lemma]
    Given some $X\subseteq V$ with $u\in \text{Span}(X)$ but $u\not\in \text{Span}(X\setminus \{v\})$ for some $v\in X$, let $Y=(X\setminus \{v\})\cup \{u\}$ then $\text{Span}(X) = \text{Span}(Y)$.
\end{lemma}

\begin{theorem}
    Given a LI $S\subseteq V$ and spanning set $T\subseteq V$, $|S|\leq|V|$.
\end{theorem}

\begin{corollary}
    If $S$ and $T$ are both bases for $V$, $|S|=|T|$.
\end{corollary}

\subsection{Dimension}

\begin{definition}[Dimension of a vector space]
    If $V$ is finite dimensional then the \textbf{dimension} of $V$, $\text{dim}V$, is the size of any basis of $V$.
\end{definition}

\begin{definition}[Notable subspaces]
    Let $U$ and $W$ both be subspaces of $V$, the \textbf{intersection} of $U$ and $W$: \[
        U\cup W := \{v\in V: v\in U  \ \text{\textcolor{black}{ and }} v\in W\}
    \]
    is a subspace of $V$, and the \textbf{sum} of $U$ and $W$: \[
        U + W := \{u + W: u\in U, w\in W\}
    \]
    is also a subspace of $V$.
\end{definition}

\begin{theorem}
    Let $U$ and $W$ both be subspaces of $V$, we have: \[
        \text{dim}(U+W)=\text{dim}U+\text{dim}W-\text{dim}(U\cap W)
    \textcolor{black}{.}
    \]
\end{theorem}

\section{Matrix rank}
\begin{definition}
    Given a field $\FF$ and a matrix $A\in M_{m\times n}(\FF)$ we have: \begin{itemize}
        \item the \textbf{row space} of $A$, $\text{RSp}(A)$, as the span of the rows of $A$, this is a subspace of $\FF^n$,
        \item the \textbf{row rank} of $A$, is $\text{dim}(\text{RSp}(A))$,
        \item the \textbf{column space} of $A$, $\text{CSp}(A)$, as the span of the columns of $A$, this is a subspace of $\FF^m$,
        \item the \textbf{column rank} of $A$, is $\text{dim}(\text{CSp}(A))$.
    \end{itemize}
\end{definition}

\begin{theorem}
    For any matrix $A$, the row rank of $A$ is equal to the column rank of $A$.
\end{theorem}

\begin{definition}[Rank of a matrix]
    The \textbf{rank} of a matrix $A$, $\text{rank}(A)$, is equal to the row/column rank of $A$.
\end{definition}

 \begin{theorem}
     Given a field $\FF$ and a matrix $A\in M_{n\times n}(\FF)$ with $\text{rank}(A)=n$: \begin{itemize}
         \item the rows of $A$ for a basis for $\FF^n$,
         \item the columns of $A$ for a basis for $\FF^n$,
         \item $A$ is invertible.
     \end{itemize}
 \end{theorem}


\section{Linear transformations}

\subsection{Definition}

\begin{definition}[Linear transformation]
    Given $\FF$-vector spaces $V$ and $W$, let $T:V \rightarrow W$ be a function, $T$ is a \textbf{linear transformation} iff the following two properties hold: \begin{enumerate}
        \item $T$ \textbf{preservers vector addition}: $\forall v_1,v_2\in V$ we have $T(v_1 +_V v_2) = T(v_1) +_W T(v_2)$,
        \item $T$ \textbf{preservers scalar multiplication}: $\forall v\in V$ and  $\forall \lambda \in \FF$ we have $\lambda T(v) = T(\lambda v)$.
    \end{enumerate} 
\end{definition}

\begin{definition}[Identity transformation]
    The \textbf{identity transformation} of the vector space $V$ is the linear transformation $\text{Id}_V: V\rightarrow V$ with $\text{Id}_V(v):=v$ for all $v\in V$.
\end{definition}

\begin{definition}[Linear transformation of a matrix]
    If $A\in M_{m\times n}(\FF)$ then we can define $T:\FF^n \rightarrow \FF^m$ by $T(v):=Av$, $T$ is a linear transformation.
\end{definition}

\begin{theorem}
    If $V$ and $W$ are $\FF$-vector spaces, $T(0_V) = 0_W$ and \[
        v = \lambda_1v_1 + \ldots + \lambda_nv_n \iff T(v) = \lambda_1T(v_1) + \ldots + \lambda_nT(v_n)
        \textcolor{black}{.}
    \]
\end{theorem}

\begin{theorem}
    If $V$ and $W$ are $\FF$-vector spaces, $\Hom(V,W)$ is the set of linear transormations from $V$ to $W$, with pointwise addition and scalar multiplication $\Hom(V,W)$ is a $\FF$-vector space.
\end{theorem}



\subsection{Image and kernel}
Throughout, assume $T:V\rightarrow W$ is a linear transformation and $V,W$ are $\FF$-vector spaces
\begin{definition}[Image]
    We define the \textbf{image} of $T$, denoted $\text{Im }T$, as \[
        \text{Im }T := \{w\in W: \exists v\in V, T(v)=W\}
        \textcolor{black}{,}
    \]
    with $\text{Im }T$ being a subspace of $W$.
\end{definition}

\begin{definition}[Kernel]
    We define the \textbf{kernel} of $T$, denoted $\text{ker}T$, as \[
        \text{ker }T := \{v\in V: T(v)=0_W\}
        \textcolor{black}{,}
    \]
    with $\text{ker }T$ being a subspace of $V$.
\end{definition}

\begin{theorem}
    If $v_1,v_2\in V$ then $T(v_1)=T(v_2) \iff v_1-v_2\in\text{ker }T$.
\end{theorem}

\begin{theorem}
    If $\{v_1,v_2,\ldots,v_n\}$ is a basis for $V$, then $\text{Im }T = \text{Span}(\{T(v_1),T(v_2),\ldots,T(v_n)\})$.
\end{theorem}

\begin{remark}
    If $T$ is the linear transformation for some matrix $A\in M_{m\times n}(\FF)$ then, $\text{ker} T$ is the set of solutions for $Av=0$, $\text{Im }T$ is the column space of $A$, and $\text{dim}(\text{Im }T)=\text{rank }A$
\end{remark}

\subsection{Rank nulity}

\begin{theorem}[Rank Nulity Theorem]
    If $V$ and $W$ are finite dimensional $\FF$-vector spaces and $T:V\rightarrow W$ is a linear transformation, we have: \[
        \text{dim}(\text{Im }T) + \text{dim}(\text{ker }T) = \text{dim }V
    \textcolor{black}{.}
    \]
\end{theorem}

\section{Representations}

\subsection{Matrices of transformations}
Throughout this subsection let $V$ be an $n$-dimensional $\FF$-vector space and $B=\{e_1, e_2, \ldots, e_n\}$ be a basis for $V$.

\begin{definition}[Reperesentation of a vector]
    Given some $v\in V$ with $v = \lambda_1v_1 + \lambda_2v_2 + \dots + \lambda_nv_n$ for $\lambda_i\in\FF$, we define the $v$ \textbf{with respect to} (\textbf{w.r.t.}) $B$ as \[
     [v]_B := \begin{pmatrix}
         \lambda_1 \\
         \lambda_2 \\
         \vdots \\
         \lambda_n
     \end{pmatrix} \in \FF^n
     \textcolor{black}{.}
    \]
\end{definition}

\begin{remark}
    This must be well defined as all vectors have a unique representation in terms of every basis.
\end{remark}

\begin{definition}[Linear isomorphism]
    A \textbf{linear isomorphism} is a bijective linear transformation.
\end{definition}

\begin{theorem}
    The linear transformation $T:V\rightarrow\FF^n$ given by $T(v):= [v]_B$ is a linear isomorphism.
\end{theorem}

\subsection{Matrices of transformations}
\begin{definition}[Representation of a linear transformation]
    Given finite dimensional $\FF$-vector spaces $V$ and $W$ with bases $B = \{v_1,v_2,\ldots,v_n\}$, $C = \{w_1,w_2,\ldots,w_n\}$ respectively, the \textbf{matrix of $T$ w.r.t. $B$ and $C$} denoted $_C[T]_B$ is $m\times n$ matrix with the $i$th column given by $[T(v_i)]_C$. $_B[T]_B$ is often shortened to $[T]_B$.
\end{definition}

\begin{remark}
    $_C[T]_B[v]_B = [T(v)]_C$, for all $v\in V$.
\end{remark}

\begin{theorem}
    Given a finite dimensional $\FF$-vector space $V$ with bases $B = \{v_1,v_2,\ldots,v_n\}$ and $C = \{w_1,w_2,\ldots,w_n\}$, if $v_i = \lambda_{1i}w_1 + \lambda_{2i}w_2 + \ldots + \lambda_{ni}w_n$ and $P$ is the matrix given by $P=[\lambda_{ij}]_{n\times n}$, we have: \begin{itemize}
        \item $P=[X]_C$ where $X$ is the unique linear transformation given by $X(w_i)=v_i$ for all $i\in[1,n]$,
        \item $P([v]_B) = [v]_C$ for all $v\in V$,
        \item $P= {}_C[\text{Id}_V]_B$.
    \end{itemize}
    $P$ is often also called the \textbf{change of basis matrix} from $B$ to $C$.
\end{theorem}

\begin{corollary}
    $P$ is invertible with $(P)^{-1}= ({}_C[\text{Id}_V]_B)^{-1} = {}_B[\text{Id}_V]_C$. 
\end{corollary}

\begin{theorem}
    If $T:V\rightarrow V$ is a linear transformation $[T]_C = ({}_C[\text{Id}_V]_B)[T]_B({}_B[\text{Id}_V]_C)$.
\end{theorem}

\section{Determinants}

\subsection{Definition}

\begin{definition}[Minor of matrix]
    Given a matrix $A\in M(\FF)_n$ the \textbf{$ij$th-minor} of the matrix $A$, $A_{ij}\in M(\FF)_n$, is $A$ with row $i$ and column $j$ removed.
\end{definition}

\begin{definition}[Determinant]
    The \textbf{determinant} of the matrix $A$ is defined recursively by \[
    \text{det}(A) := 
    \begin{dcases}
    \omit\hfil $a_{11}$\hfil & \textcolor{black}{\text{if }} A \textcolor{black}{\text{ is a matrix with a single row and column }}\\
    \sum_{j=1}^n (-1)^{j+1} a_{1j} \text{det}(A_{1j}) & \textcolor{black}{\text{otherwise.}}
    \end{dcases} 
    \textcolor{black}{.}
    \]
    The determinant is only a function on square matrices.
\end{definition}

\begin{theorem}
    If a matrix $A$ is singular, $\text{det}(A)=0$.
\end{theorem}

\begin{theorem}
    If $A$ is invertible then the columns of $A$ are LI.
\end{theorem}

\subsection{Properties}

\begin{definition}[EROs]
    The three ERO's on the matrix $A$ to form $A'$ have the following effects on the determinant: \begin{itemize}
        \item multiplying a row by $\lambda\neq0$, $\text{det}(A') = \lambda\text{det}(A)$;
        \item swapping two rows, $\text{det}(A') = -\text{det}(A)$;
        \item adding a scalar multiple of one row to another, $\text{det}(A') = \text{det}(A)$.
    \end{itemize}
\end{definition}

\begin{definition}[Other miscellaneous properties] For obvious types:
\begin{itemize}
    \item If $A$, $B$ and $C$ all only differ in the $i$th row with the $i$th row of $C$ being the sum of the $i$th row of $A$ and $B$, $\text{det}(C) = \text{det}(A) + \text{det}(B)$,
    \item if a matrix $A$ has two identical rows, $\text{det}(A)=0$,
    \item $\text{det}(AB)=\text{det}(A)\text{det}(B)$,
    \item $\text{det}(A^\T) = \text{det}(A)$,
    \item $\text{det}(I_n)=1$.
\end{itemize}
\end{definition}

\begin{definition}[Cofactor]
    The $ij$th cofactor of a matrix $A$ is defined as, \[
        c_{ij} := (-1)^{i+j}\text{det}(A_{ij})
        \textcolor{black}{.}
    \]
    The \textbf{matrix of cofactors} of $A$ is defined as $C=[c_{ij}]_{n\times n}$ where $c_{ij}$ is the $ij$th cofactor of $A$.
\end{definition}

\begin{theorem}
    For a matrix $A$ with matrix of cofactors $C$, $C^\T A = \text{det}(A)\I_n$.
\end{theorem}

\begin{theorem}[Cramer's Rule]
    ugh
\end{theorem}

\begin{definition}
    The \textbf{determinant} of a linear transformation $T:V\rightarrow V$ where $B$ is a basis for $T$, $\text{det}(T) = \text{det}([T]_B)$. This definition says, rather importantly, that the determinant of the matrix of linear transformation is independent of the basis that linear transformation is represented in.
\end{definition}

\section{Eigen-things}
The prefix ``eigen" comes from the German word ``eigen" which can be roughly translated to mean ``proper" or ``characteristic".

\subsection{Eigenvectors and eigenvalues}

\begin{definition}[Eigenvectors and eigenvalues]
    Given the finite dimensional $\FF$-vector space, $V$, and the linear transformation $T:V\rightarrow V$, we say $v\in V\setminus{\{0_V\}}$ is an \textbf{eigenvector} of $T$ if it satisfies the equation $T(v)=\lambda v$ for some $\lambda \in \FF$, we call $\lambda$ the corresponding \textbf{eigenvalue}.
\end{definition}

\begin{definition}[Eigenspace]
    The \textbf{eigenspace} of an eigenvalue of a given linear transformation $T:V\rightarrow V$ is the set of eigenvectors that correspond to said eigenvalue. The eigenspace of any eigenvalue $\lambda$  of $T$ is a subspace of $V$.
\end{definition}

\begin{remark}
    The eigenvectors, eigenvalues and eigenspaces of a matrix are defined obviously and do not depend on which basis the linear transformation is respresented in.
\end{remark}

\subsection{Characteristic polynomial}

\begin{theorem}
    If $D$ is a square diagonal matrix, $D^k$ is $D$ with its entries raised to the power of $k$.
\end{theorem}

\begin{definition}[Characteristic polynomial]
    Given the finite dimensional $\FF$-vector space $V$ with basis $B$ and the linear transformation $T:V\rightarrow V$, we define the \textbf{characteristic polynomial} of $T$, $\chi_T:\FF\rightarrow\FF$ by $\chi_T(\lambda):= \text{det}(\lambda I_n-T_B)$.
\end{definition}

\begin{theorem}
    The characteristic polynomial of a linear transformation is independent of the basis it is represented in.
\end{theorem}

\begin{remark}
    Therefore, the characteristic polynomial of a matrix can be defined as the characteristic polynomial of the linear transformation it represents.
\end{remark}

\subsection{Diagonalisation}
\begin{definition}[Diagonalisability]
    Given a finite dimensional $\FF$-vector space $V$, a linear transformation $T:V\rightarrow V$ is \textbf{diagonalisable} if there exists a basis for $V$ consisting of eigenvectors of $T$. Similarly the matrix $A\in M_n(\FF)$ is \textbf{diagonalisable} if there exists a basis to $\FF^n$ as eigenvectors of $A$.
\end{definition}

\begin{theorem}
    If $V$ is a n-dimensional vector space and $T:V\rightarrow V$ has $n$ distince eigenvalues, $T$ is diagonalisable.
\end{theorem}

\begin{theorem}
    If a matrix $A\in M_n(\FF)$ is diagonalisably,  let $P$ be the matrix with columns as eigenvectors of $A$ and $D$ be the diagonal matrix with $ii$th entry as the corresponding eigenvalue for the $i$th column of $P$, then $A=PDP^{-1}$.
\end{theorem}

\section{Orthogonality}

\subsection{Inner product spaces*}

\begin{definition}[Inner product]
    Let $V$ be an $n$-dimensional $\FF$-vector space, a \textbf{inner product} on $V$ is a bilinear map $\langle\cdot,\cdot\rangle: V \times V \rightarrow \FF$ satisfying the following: \begin{itemize}
        \item $\langle u,v \rangle=\langle v,u \rangle$ for all $u,v\in V$ (Symmetry),
        \item $\langle u+v,w \rangle=\langle u,w \rangle+\langle v,w \rangle$ and $\langle \lambda u,v \rangle=\lambda\langle u,v \rangle$ for all $u,v,w\in V$ and $\lambda\in\FF$ (Bilinearity),
        \item $\langle v,v \rangle\geq 0$ for all $v\in V$ with equality when $v=0_V$ (Positive-definite).
    \end{itemize}
    Bilinearity must hold in both arguments however it can be derived from a single argument with the symmetry propoerty.
\end{definition}

\begin{definition}[Norm]
    Given a real-vector space $V$ with an inner product $\langle\cdot,\cdot\rangle$ the \textbf{norm} induced by the inner product of $v\in V$ is: \[
        ||v|| := \sqrt{\langle v,v\rangle}
    \]
\end{definition}

\begin{definition}[Orthogonality]
    Two vectors $u,v$ in an real or complex vector space $V$  with inner product $\langle\cdot,\cdot\rangle$ are \textbf{orthogonal} iff: $\langle u, v\rangle=0$.
\end{definition}

\subsection{Orthonormal sets}
Throughout the remainder of this section we will assume all vectors spaces are over the real or complex numbers and will use the dot product as our inner product with its induced norm.

\begin{definition}[Orthogonal sets]
    A set of vectors $\{v_1,v_2,\ldots,v_n\}$ in a vector space is \textbf{orthogonal} if it is pariwise orthogonal.
\end{definition}

\begin{definition}[Orthonormal sets]
    A set of vectors $\{v_1,v_2,\ldots,v_n\}$ in a vector space is \textbf{orthonormal} if it orthogonal and satisfies $||u_i||=1$ for all $i\in[1,n]$.
\end{definition}

\begin{theorem}
    The columns of an orthogonal matrix $P\in M_n(\RR)$ form an orthonormal set.
\end{theorem}

\subsection{Gramm-Schmidt process}
The Gramm-Shmidt process is a method of producing orthonormal bases.
\begin{algorithm}[Gramm-Shmidt process]
    Given a LI set $\{v_1,v_2,\ldots,v_r\} \in \RR^n$ the \textbf{Gramm-Shmidt process} will produce the set of vectors $\{w_1,w_2,\ldots,w_r\} \in \RR^n$ by the following:
    \begin{samepage}
    \begin{enumerate}
        \item[] $\displaystyle w_1 = v_1$,
        \item[] $\displaystyle w_2 = v_2 - \frac{w_1\cdot v_2}{||w_1||^2}w_1$,
        \item[] $\displaystyle w_3 = v_3 - \left(\frac{w_1\cdot v_3}{||w_1||^2}w_1 + \frac{w_2\cdot v_3}{||w_2||^2}w_2\right)$,
        \item[] $\hspace{16.5pt}\vdots$
        \item[] $\displaystyle w_r = v_r - \sum_{j=1}^{r-1}\frac{w_j\cdot v_r}{||w_j||^2}w_j$.
    \end{enumerate}
    \end{samepage}
    Note that each vector is the original vector $v_i$ with its projection along all of the previous $w_j$s subtracted and therefore $\{w_1,w_2,\ldots,w_r\}$ is orthogonal. Finally, $\{u_1,u_2,\ldots,u_r\}$, where $\displaystyle u_i = \frac{w_i}{||w_i||}$ for all $i\in[1,r]$, is an orthonormal set with $\text{Span}(\{u_1,u_2,\ldots,u_r\}) = \text{Span}(\{v_1,v_2,\ldots,v_r\})$.
\end{algorithm}

\begin{corollary}
    Given some vector $u\in\RR^n$ there exists an orthogonal matrix in $M_n(\RR)$ with $u$ as its first column.
\end{corollary}

\section{Real symmetric matrices}
Throughout this section, unsurprisingly, all matrices will be assumed to be real.
\subsection{Introduction}

\begin{definition}[Self-adjoint matrices]
    If a matrix $A\in M_n(\RR)$ is symmetric and satisfies $A(u\cdot v) = (Au)\cdot v$ for all vectors $u,v\in\RR^n$, we say $A$ is \textbf{self-adjoint} w.r.t. the usual scalar product.
\end{definition}

\begin{theorem}
    If $A\in M_n(\RR)$ is symmetric with $\lambda\in\CC$ a root of $\chi_A(x)=0$, $\lambda\in\RR$.
\end{theorem}

\begin{corollary}
    Real symmetric matrices have at least 1 real eigenvalue.
\end{corollary}

\begin{theorem}
    If $A\in M_n(\RR)$ is symmetric with discrete eigenvalues $\lambda,\mu\in\RR$, their corresponding eigenvectors $u,v\in\RR^n$ satisyf $u\cdot v=0$.
\end{theorem}

\subsection{Spectral theorem}
    
\begin{theorem}[Spectral theorem]
    If $A\in M_n(\RR)$ is symmetric, then there exists an orthonormal matrix $P$ such that $P^{-1}AP$ is diagonal.
\end{theorem}

\begin{corollary}
    Appropriately scaled eigenvectors of a symmetric matrix $A\in M_n(\RR)$ form an orthonormal basis for $\RR^n$.
\end{corollary}

\end{document}