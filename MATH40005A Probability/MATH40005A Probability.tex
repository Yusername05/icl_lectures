
\def\module{MATH40005A Probability}
\def\lecturer{Professor Almut Veraart}
\def\term{Autumn 2023}
\def\cover{
$$
\begin{tikzpicture}[scale=1.3]
    \draw[black] (4,0) -- ({2*sqrt(2)},{-2*sqrt(2)});
    \draw[black] (0,4) arc (-37.5:-97.5:3.06);
    \draw[black] (-4,0) arc (82.5:-37.5:1.77);
    \draw[black] (0,4) arc (120:-30:2.93);
    \draw[black] ({2*sqrt(2)},{-2*sqrt(2)}) arc (-15:-165:2.94);
    \draw[black] ({-2*sqrt(2)},{2*sqrt(2)}) arc (232.5:307.5:4.65);
    \draw[black] (-4,0) arc (-100:-34.8:6.875);
    
    \draw[black, densely dotted] ({2*sqrt(2)},{2*sqrt(2)}) -- (4,0);
    \draw[black, densely dotted] (-4,0) arc (52.5:-7.5:3.06);
    \draw[black, densely dotted] (0,4) arc (112.5:22.5:2.175);
    \draw[black, densely dotted] (0,-4) arc (22.5:112.5:2.175);
    \draw[black, densely dotted] (0,-4) arc (157.5:67.5:2.175);
    \draw[black, densely dotted] (-4,0) arc(202.5:112.5:2.17);
    \draw[black, densely dotted] (0,4) arc (52.5:172.5:1.77);
    \draw[black, densely dotted] (4,0) arc (97.5:217.5:1.77);
    
    \draw[black, densely dashed] (0,4) -- ({2*sqrt(2)},{2*sqrt(2)});
    \draw[black, densely dashed] (-4,0) arc (172.5:232.5:3.06);
    \draw[black, densely dashed] (4,0) arc (247.5:157.5:2.175);
    \draw[black, densely dashed] (0,-4) arc (-112.5:-22.5:2.175);
    \draw[black, densely dashed] (4,0) arc (37.5:-82.5:1.77);
    \draw[black, densely dashed] (0,4) arc (-7.5:-127.5:1.77);
    \draw[black, densely dashed] ({-2*sqrt(2)},{-2*sqrt(2)}) arc (-98:-262:2.86);
    \draw[black, densely dashed] (0,-4) arc (7.5:82.5:4.65);
    
    \draw[black, densely dashdotted] ({2*sqrt(2)},{-2*sqrt(2)}) -- (0,-4);
    \draw[black, densely dashdotted] (-4,0) -- ({-2*sqrt(2)},{2*sqrt(2)});
    \draw[black, densely dashdotted] (0,4) arc (82.5:142.5:3.06);
    \draw[black, densely dashdotted] (4,0) arc (127.5:187.5:3.06);
    \draw[black, densely dashdotted] (4,0) arc (-22.5:67.5:2.175);
    \draw[black, densely dashdotted] (0,-4) arc (-67.5:-157.5:2.175);
    \draw[black, densely dashdotted] (0,4) arc (52.5:262.5:3.825);
    \draw[black, densely dashdotted] (-4,0) arc (-82.5:-52.8:14.4);
    
    \draw[black, densely dashdotdotted] ({-2*sqrt(2)},{2*sqrt(2)}) -- (0,4);
    \draw[black, densely dashdotdotted] (4,0) arc (7.5:-52.5:3.06);
    \draw[black, densely dashdotdotted] (0,4) arc (202.5:292.5:2.175);
    \draw[black, densely dashdotdotted] (-4,0) arc (-67.5:22.5:2.175);
    \draw[black, densely dashdotdotted] (-4,0) arc (142.5:262.5:1.77);
    \draw[black, densely dashdotdotted] (4,0) arc (37.5:-172.5:3.84);
    \draw[black, densely dashdotdotted] ({2*sqrt(2)},{2*sqrt(2)}) arc (142.5:217.5:4.65);
    
    \fill (4,0) circle (0.05) node[right]{A};
    \fill ({2*sqrt(2)},{2*sqrt(2)}) circle (0.05) node[above right]{B};
    \fill (0,4) circle (0.05) node[above]{C};
    \fill ({-2*sqrt(2)},{2*sqrt(2)}) circle (0.05) node[above left]{D};
    \fill (-4,0) circle (0.05) node[left]{E};
    \fill ({-2*sqrt(2)},{-2*sqrt(2)}) circle (0.05) node[below left]{F};
    \fill (0,-4) circle (0.05) node[below]{G};
    \fill ({2*sqrt(2)},{-2*sqrt(2)}) circle (0.05) node[below right]{H};
\end{tikzpicture}
$$

\begin{center}
    A minimum vertex 5-Venn diagram.
\end{center}
}
\def\syllabus{Basic set theory and combinatorics, Kolmogorov probability axioms, Conditional probability and independence, DRVs, CRVs, Transformations, Expectaction, Variance, Multivariate calculus, Joint Distributions, LOTUS, pgfs, mgfs, conditional distribution}
\def\thm{subsection}

\input{../style/header}

\begin{document}

\input{../style/cover}

\section{Introduction}

\lecture{1}{Monday}{30/10/2023}

The following are complementary reading for the course.
\begin{itemize}
\item G. Grimmett and D. J. A. Welsh, Probability: An Introduction, 1986
\item J. K. Blitzstein and J. Hwang, Introduction to Probability, 2019
\item D. F. Anderson et al, Introduction to Probability, 2018
\item S. M. Ross, Introduction to Pro ability Models, 2014
\item G. Grimmett and D. Stirzaker, Probability and Random Processes, 2001
\item G. Grimmett and D. Stirzaker, One Thousand Exercises in Probability, 2009
\end{itemize}

These notes are written assuming all material covered in ICL's \textbf{MATH40001}, \textbf{MATH40002} and \textbf{MATH40004}.

\begin{notation*}
    Denote the natural numbers by $\NN = \{1,2,...\}$ and define $\NN_0 := \NN \cup \{0\}$. Moreover, denote the integers by $\ZZ = \{...,-2,-1,0,1,2,...\}$ and the reals by $\RR$. For real numbers $a < b$ write $[a,b]$ and $(a,b)$ for closed and open intervals respectively.
\end{notation*}

\section{Sample spaces and interpretations of probability}

\subsection{Sample spaces and set theory}

\begin{definition}
    The \textbf{sample space} $\Omega$ is the set of all possible outcomes of an experiment. An element of the sample space $\omega \in \Omega$ is a \textbf{sample point}.
\end{definition}

\begin{examples}
    When flipping a coin $\Omega  = \{H,T\}$. When rolling a standard die $\Omega = \{1,2,3,4,5,6\}$.
\end{examples}

\begin{definition}
    Subsets of $\Omega$ are collections of sample points and called \textbf{events}.
\end{definition}

Suppose events $A,B\subseteq\Omega$:
\begin{itemize}
    \item $A\cup B$ is the event that $A$ or $B$ or both occur.
    \item $A\cap B$ is the event that $A$ and $B$ both occur.
    \item $A^c = \bar{A}$ is the event that occurs iff $A$ does not occur.
\end{itemize}


Let $\III$ be a general index set with $A_i \subseteq\Omega,\; \forall i \in\III$ and $B \subseteq \Omega$. The following identities hold.
\[
\left( \bigcup_{i\in\III}A_i \right)^c = \bigcap_{i\in\III}A_i^c \textcolor{black}{,} \quad
\left( \bigcap_{i\in\III}A_i \right)^c = \bigcup_{i\in\III}A_i^c \textcolor{black}{,} \qquad
B \cap \left( \bigcup_{i\in\III}A_i \right) = \bigcap_{i\in\III}(A_i\cup B) \textcolor{black}{,} \quad
B \cup \left( \bigcap_{i\in\III}A_i \right) = \bigcup_{i\in\III}(A_i\cap B) \textcolor{black}{.}
\]
These are \textbf{De Morgan's Laws} and \textbf{Distributivity} respectively.

\lecture{2}{Tuesday}{31/10/2023}

\subsection{Interpretation of probability}
\begin{definition}
    The \textbf{Cardinality} of a set, denoted $\text{card}(A)$ or $|A|$ is the number of elements in the set $A$.
\end{definition}

\begin{definition}
    Two sets have the same cardinality iff there exists a bijection between the them.
\end{definition}

\begin{definition}
    $A$ is \textbf{finite} if it has as finite numbers of elements, $A$ is \textbf{countably infinite} if there exists a bijection $f: A \to \NN$, $A$ if \textbf{countable} if it is finite or countable infinite, $A$ is \textbf{uncountable} or \textbf{uncountable infinite} if it isn't countable.
\end{definition}

Samples spaces can be countable or uncountable.

\begin{definition}[Naive probability]
    Suppose $|A| < \infty$ and we want to assign a probability to $A \subseteq \Omega$.
    \[
    \P_{Naive}(A) := \frac{|A|}{|\Omega|} \implies \P(A^c) = 1 - \P(A)  \textcolor{black}{.}
    \]
    This Naive example does not consider when $|A|$ is infinite but of finite area.
\end{definition}
\begin{example}
    Let $\Omega = \{(x,y) \in \RR^2, x^2+y^2=1\}$ and $A\subseteq\Omega$. Define: 
    \[
    \P(A) := \frac{\text{area of } A}{\text{area of } \Omega}
    \]
    In the case where $A = \{(x,y) \in \RR^2, x^2+y^2=0.5^2\}$ we have $\P(A) = 0.25$
\end{example}
\begin{remark}
    For classical / naive probability we require $|\Omega| < \infty$ or the ``area" of $\Omega$ be finite. 
\end{remark}

\begin{definition}[Limiting frequency]
    Consider $n_{total}$ repetitions of an experiment and $n_A$ the number of time $A$ occurs.
    \[\P(A) := \lim_{n_{total} \to \infty} \frac{n_A}{n_{total}}\]
    Unfortunately, $n_{total} \to \infty$ is often hard to conceive with finite representations not necessarily being representative.
\end{definition}

\begin{definition}[Subjective probability]
    For an event $A$ assign the probability $\P(A)$ based on our own personal beliefs. The subjective probability need not be the same for different individuals, and despite its appearance it remains a valid interpretation of probability.
\end{definition}

\begin{remark}
    All three interpretations of probability depend of assumptions about the experiment.
\end{remark}

\lecture{3}{Friday}{03/11/2023}

\section{Counting}
\subsection{Multiplication principle}
Computing naive probabilities often requires some combinatorics.

\begin{definition}[Multiplication  principle]
    If we perform an experiment $A$ that has $a$ possible outcomes and an experiment $B$ with $b$ possible outcomes (in any order) then the number of outcomes of the  \textbf{compound experiment} will be $ab$.
\end{definition}

\begin{remark}
    When dealing with repetitions of the same experiment (with sample space $\Omega$, the sample space is given by the Cartesian product of the individual samples spaces.
    \[
        \Omega_1 \times \Omega_2 \times \cdots \times \Omega_n :=  \{(\omega_1,\omega_2,\ldots,\omega_n): \omega_i \in \Omega_i\}\textcolor{black}{.}
    \]
    The cardinality of this samples space follows from the multiplication principle.
\end{remark}

\subsection{Power sets}
\begingroup\belowdisplayskip=-10pt
    \begin{definition}[Power Set]
        Given a set $A$ its \textbf{power set} is defined as:
        \[
            \PPP(A) := \{X:X\subseteq A\}\textcolor{black}{.}
        \]
    \end{definition}
\endgroup

\begin{theorem}
    If $A$ is a finite set, $|\PPP(A)|=2^{|A|}$.
\end{theorem}

\subsection{Combinatorial coefficients}

\begingroup\belowdisplayskip=-0pt
    \begin{definition}[Factorial]
        Let $n\in\NN$ the \textbf{factorial} of $n$ is defined as:
        \[
        n! := \prod_{i=1}^ni\textcolor{black}{.}
        \]
    \end{definition}
\endgroup

\begingroup\belowdisplayskip=-10pt
    \begin{definition}[Descending factorial]
        Let $k,n\in\NN$ with $k\leq n$ the \textbf{descending factorial} denoted $(n)_k$ is defined as:
        \[
        (n)_k := n(n-1)\ldots(n-k+1) = \prod_{i=0}^{k-1}(n-i) = \prod_{j=n-k+1}^{n}j = \frac{n!}{(n-k)!}\textcolor{black}{.}
        \]
    \end{definition}
\endgroup

\begingroup\belowdisplayskip=-10pt
    \begin{definition}[Binomial coefficient]
        Let $k,n\in\NN_0$ the \textbf{binomial coefficient} is the number of subsets of size $k$ of a set $n$:
        \[ 
            \binom{n}{k} := 
            \begin{dcases}
            \frac{n(n-1)\ldots(n-(k-1))}{k!} = \frac{(n)_k}{k!} = \frac{n!}{(n-k)!k!} & \textcolor{black}{\text{if }} k\leq n \\
            \omit\hfil0\hfil & \textcolor{black}{\text{otherwise.}}
            \end{dcases}
        \]
    \end{definition}
\endgroup

\subsection{Sampling with and without replacement}
``Definitions" given in the context of drawing balls from an urn, $S=\{1,2,\ldots,n\}$.
\begin{definition}[Ordered sampling with replacement]
    Take out a ball from $S$, note its number, put it back; repeat this $k$ times. The sample space for this experiment is $\Omega = S^k$.
\end{definition}

\begin{definition}[Ordered sampling without replacement]
    Take out a ball form $S$, note its number but \textbf{do not} put it back; repeat $k<n$ times. There are $|\Omega| = (n)_k$ possible outcomes.
\end{definition}

\begin{definition}[Unordered sampling without replacement]
    We take $k$ balls out of the urn, there are $\binom{n}{k}$ possibilities.
\end{definition}

\begingroup\belowdisplayskip=-10pt
    \begin{definition}[Unordered sampling with replacement]
        We take $k$ balls out of the urn, with the stars and bars argument: there must be $k$ stars divided by $n-1$ bars giving us:
        \[
        |\Omega| = \binom{n+k-1}{k} = \binom{n+k-1}{n-1}\textcolor{black}{.}
        \]
    \end{definition}
\endgroup

\section{Axiomatic probability}
\subsection{Event space}
We do not always want to consider all subsets of $\Omega$ so denote $\FFF \subseteq \PPP(\Omega)$ the \textbf{event space}, which contains the events we are allowed to consider. $\FFF$ must always be a $\sigma$-algebra.

\begin{definition}[Algebra]
    $\FFF$ is an \textbf{algebra} (or a field) on $\Omega$ iff:\\
    \begin{enumerate*}
        \item $\emptyset \in \FFF$. \hspace{100pt}
        \item $A \in \FFF \implies A^c \in \FFF$. \hspace{100pt}
        \item $A,B \in \FFF \implies A\cup B \in \FFF$.
    \end{enumerate*}
\end{definition}

\begin{definition}[$\sigma$-algebra]
    $\FFF$ is a \textbf{$\sigma$-algebra} (or a $\sigma$-field) on $\Omega$ iff:\\
    \begin{enumerate*}
        \item $\emptyset \in \FFF$. \hspace{15pt}
        \item $A \in \FFF \implies A^c \in \FFF$. \hspace{15pt}
        \item For all $i$ in some countable indexing set $\III$, $A_i \in \FFF \implies \bigcup\limits_{i\in \III}A_i \in \FFF$.
    \end{enumerate*}
\end{definition}

\begin{remark}
    \begin{enumerate}
        \item Any algebra is closed under finite unions and finite intersections.
        \item Any $\sigma$-algebra is closed under countable intersections.
        \item Any ($\sigma$-)algebra on $\Omega$ contains $\Omega$.
    \end{enumerate}
\end{remark}

\begin{definition}[Trivial sigma algebra]
    The \textbf{trivial sigma algebra} on $\Omega$ is defined as $\FFF_{trivial} := \{ \emptyset, \Omega \}$.
\end{definition}

\begin{example}[Smallest $\sigma$-algebra of an element]
    For some $A \subseteq \Omega$, the sigma algebra $\FFF_A := \{\emptyset, A, A^c , \Omega \}$ is the smallest $\sigma$-algebra on $\Omega$ (smallest cardinality) that contains $A$.
\end{example}

\subsection{Probability measure}

\begingroup\belowdisplayskip=-10pt
    \begin{definition}[Probability measure]
        A mapping $\P: \FFF \to \RR$ is a \textbf{probability measure} on $(\Omega, \FFF)$ iff:\\
        \begin{enumerate*}
            \item $\P(A)\geq0$ for all $A\in\FFF$.
            \item $\P(\Omega) = 1$.
            \item For a countable, disjoint sequence of events $(A_i)_{i\in\III}$ on an indexing set $\III$: \\
        \end{enumerate*}
        \[
        \P\left(\bigcup_{i\in\III}A_i\right) = \sum_{i\in\III}\P(A_i)\textcolor{black}{.}
        \]
    \end{definition}
\endgroup

\begingroup\abovedisplayskip=-10pt 
\subsection{Probability space}
\endgroup
\begin{definition}[Probability space]
    A \textbf{probability space} is a triple $(\Omega, \FFF, \P)$, with $\Omega$ a sample space, $\FFF$ a $\sigma$-algebra on $\Omega$, and $\P$ a probability measure on $(\Omega, \FFF)$.
\end{definition}

\begin{corollary} For $A,B \in \FFF$: \newline
    \begin{enumerate*}
        \item $\P(A^c) = 1 - \P(A)$. \hspace{25pt}
        \item $A \subseteq B \implies \P(A) \leq \P(B)$. \hspace{25pt}
        \item $\P(A\cup B) = \P(A) + \P(B) - \P(A\cap B)$.
    \end{enumerate*}
\end{corollary}

\section{Conditional probability}
\begin{definition}[Conditional probability measure]
    Consider the probability space $(\Omega, \FFF, \P)$ and some event $B \in \FFF$ with $\P(B) > 0$, we construct the probability measure $\Q$ on $(\Omega, \FFF)$ by \[
        \Q(A):=\frac{P(A\cap B)}{P(B)}\textcolor{black}{.}
    \]
    Denote the \textbf{conditional probability} of $A$ given $B$ by $P(A|B) = Q(B)$.
\end{definition}

\subsection{Bayes' rule and total probability}

\begingroup\belowdisplayskip=-10pt
\begin{theorem}[Bayes' rule]
    For $A,B \in \FFF$ with $\P(A) >0, \P(B) > 0$ we have, \[
        \P(A|B) = \frac{\P(B|A)\P(A)}{\P(B)}
        \textcolor{black}{.}
        \]
\end{theorem}
\endgroup

\begin{definition}[Partition of a set]
    A partition of some set $\Omega$ is a collection $\{B_i, i \in \III\}$ for some countable index set $\III$ with $B_i \cap B_j = \emptyset$ for all $i,j\in\III$ with $i\neq j$ and $\bigcup\limits_{i\in\III}B_i = \Omega$.
\end{definition}

\begingroup\belowdisplayskip=-10pt
\begin{theorem}[Total probability]
    Given some partition $\{B_i, i\in\III\}$ of $\Omega$ with $\P(B_i)>0$ for all $i\in\III$ and some event $A\in\FFF$,\[
        \P(A)=\sum_{i\in\III}\P(A\cap B_i)=\sum_{i\in\III}\P(A|B_i)\P(B_i)\textcolor{black}{.}
    \]
\end{theorem}
\endgroup

These two theorems can then be combined to form the following.
\begingroup\belowdisplayskip=-10pt
\begin{theorem}[Bayes' rule with extra conditioning]
    For events $A,B,E \in \FFF$ with $\P(A\cap E)>0, P(B\cap E) > 0$ we have \[
    \P(A|B\cap E) = \frac{\P(B|A\cap E)\P(A|E)}{\P(B|E)}
    \textcolor{black}{.}
    \]
\end{theorem}
\endgroup

\begingroup\belowdisplayskip=-10pt
\begin{theorem}[Total probability with extra conditioning]
    Given events $A,E\in\III$  with $\P(E)>0$ and some partition $\{B_i, i\in\III\}$ of $\Omega$ with $\P(B_i\cap E)>0$ for all $i\in\III$,
    \[
        \P(A|E)=\sum_{i\in\III}\frac{\P(A\cap B_i\cap E)}{\P(E)}
        =\sum_{i\in\III}\P(A|B_i\cap E)\P(B_i| E)
        \textcolor{black}{.}
    \]
\end{theorem}
\endgroup

\section{Independence}
\subsection{Event independence}
Two events $A,B\in\FFF$ will be independent iff the occurrence of one does not effect the probability the other occurs, i.e $\P(A|B) = \P(A)$ and vice versa.

\begin{definition}[Independent events]
    Two events $A,B \in \FFF$ are said to be \textbf{independent} iff\[
    \P(A\cap B) = \P(A)\P(B) \textcolor{black}{,}
    \] and \textbf{dependent} otherwise.
\end{definition}

\begin{corollary}
    If $A$ and $B$ are independent then so are all pairs of their complements.
\end{corollary}

\begin{definition}[General independence]
    A finite collection of events $\{A_1,A_2,\ldots ,A_n\}$ is independent iff \[
    \P(A_1\cap A_2\cap \ldots \cap A_n) = \P(A_1)\P(A_2)\ldots \P(A_n)\textcolor{black}{,}
    \]
    and similarly a countably or uncountably infinite collection of events is independent iff each finite subcollection is independent.
\end{definition}

\subsection{Conditional independence}

\begingroup\belowdisplayskip=-20pt
\begin{definition}[Conditional independence]
    Given the events $A,B,C \in \FFF$ with $\P(C)>0$ we say $A$ and $B$ are \textbf{conditional independent} given $C$ iff, \[
        \P(A\cap B|C) = \P(A|C)\P(B|C) \textcolor{black}{.}
    \]
\end{definition}
\endgroup

\subsection{Product rule for general independence}
The upcoming subsection may seem disparate, they are however necessary parts to the omitted proof of the product rule for general independence and therefore deemed relevant.
\begin{definition}[Set difference]
        Given two set $A,B\in\Omega$ the \textbf{set difference} of $A$ and $B$ is defined as, $A\setminus B := A \cap B^c$.
\end{definition}

\begin{lemma}
    Any countable union of sets can be written as a countable union of disjoint sets.
\end{lemma}

\begin{definition}[Increasing and decreasing sets]
    A sequence of sets $(A_i)_{i=1}^\infty$ is said to \textbf{increase} to $A$ (written $A_i \uparrow A$) iff $A_1 \subseteq A_2 \subseteq \ldots$ and $\bigcup\limits_{i=1}^\infty = A$. The definition for a sequence of sets $(B_i)_{i=1}^\infty$ to \textbf{decrease} to a set $B$ ($B_1 \downarrow B$) is defined similarly.
\end{definition}

\begingroup\belowdisplayskip=-10pt
\begin{theorem}[Continuity property of probability measures]
    If $A_1, A_2, \ldots \in \FFF$ with $A_i \uparrow A$ or $A_i \downarrow A$ for some $A \in \FFF$, \[
        \lim_{i\rightarrow\infty}\P(A_i) = \P(\lim_{i\rightarrow\infty}A_i) = \P(A)
    \textcolor{black}{.}\]
\end{theorem}
\endgroup

\begingroup\belowdisplayskip=-20pt\abovedisplayskip=-10pt
\begin{theorem}[Product rule for general independence]
    Given a countably infinite set of independent events $A_1, A_2, \ldots \in \FFF$, \[
     \P\left(\bigcap_{i=1}^\infty{A_i}\right) = \prod_{i=1}^\infty\P(A_i)\textcolor{black}{.}
    \]
\end{theorem}
\endgroup

\section{Discrete random variables}

\subsection{Images and their properties}
throughout this subsection we will be considering the function $f:\XXX \rightarrow \YYY$.
\begin{definition}[Image]
    For some subset $A \subseteq \XXX$ we define the \textbf{image} of $A$ under $f$ by, \[
        f(A) := \{y \in \YYY: \exists x \in A, y = f(x)\} = \{f(x): x \in A\} \textcolor{black}{.}
    \]
When $A = \XXX$, $f(\XXX) = \text{Im}f$.
\end{definition}

\begin{definition}[Pre-image]
    For some subset $B \subseteq \YYY$ we now define the \textbf{pre-image} of $B$ under $f$ by, \[
    f^{-1}(B) := \{x\in\XXX: f(x) \in B\} \textcolor{black}{.}
    \]
Despite the similar notation to the inverse function of $f$ they are not the same thing. Notably, the pre-image under $f$ always exists while the inverse function need not exist.
\end{definition}

\begingroup\belowdisplayskip=-10pt
\begin{lemma}
    For a collection of subsets $B_i \in \FFF$ for $i$ in some indexing set $\III$ we have, \[
        f^{-1}\left(\bigcup_{i\in\III} B_i \right) = \bigcup_{i\in\III} f(B_i) \textcolor{black}{.}
    \]
\end{lemma}
\endgroup

\subsection{DRVs and their distributions}

\begin{definition}[Discrete random variable]
    A \textbf{discrete random variable} (\textbf{DRV}) on the probability space $(\Omega, \FFF, \P)$ is a function $X: \Omega \rightarrow \RR$ that satisfies the following properties:
    \begin{itemize}
        \item $\text{Im}X = \{X(\omega): \omega \in \Omega\}$ must be a countable subset of $\RR$.
        \item $X^{-1}(x) \in \FFF$ for all $x \in \RR$.
    \end{itemize}
\end{definition}

\begin{remark}
    The nomenclature of $X$ being discrete stems from the fact that its image is a countable subset of $\RR$ and so can be mapped to $\NN$ which we see as being discrete.
\end{remark}

\begin{definition}[Probability mass function]
    The \textbf{probability mass function} (\textbf{pmf}) of a DRV $X$ is defined as a function $p_X: \RR \rightarrow [0,1]$ such that, \[
        p_X(x) := \P(X^{-1}(x))\textcolor{black}{.}
    \]
    This is commonly denoted by $p_X(x) = \P(X=x)$.
\end{definition}

\begin{remark}
    Some useful propoerties of the pmf extending from the definition are:
    \begin{itemize}
        \item $x \not\in \text{Im}X \implies p_X(x) = 0$.
        \item For $x_1, x_2 \in \text{Im}X$ with $x_1 \neq x_2$, $X^{-1}(x_1) \cap X^{-1}(x_2) = \emptyset$.
        \item $\sum\limits_{x\in\text{Im}X}p_X(x) = \sum\limits_{x\in\RR}p_X(x) = 1$.
    \end{itemize}
\end{remark}

\begin{theorem}
    Suppose $\III$ is some indexing set and $S = \{s_i \in \RR: i\in\III\}$ is countable and $\{\pi_i:\i \in \III\}$ is a collection such that $\pi_i \geq 0$ for all $i \in \III$ and $\sum\limits_{i\in\III}\pi_i = 1$. Then there exists some probability space $(\Omega,\FFF,\P)$ and a DRV $X$ on said probability space such that $p_X(s_i)=\pi_i$ for all $i\in\III$ and $p_X(s)=0$ otherwise.
\end{theorem}

\section{Common DRVs}
\subsection{Bernoulli distribution}
All DRVs within this section will be considered over the probability space $(\Omega, \FFF, \P)$.
\begin{definition}[Bernoulli distribution]
    A DRV $X$ is said to have \textbf{Bernoulli distribtuion} with parameter $p\in(0,1)$ if $\text{Im}X = \{0,1\}$ with $p_X(1)=p$. This is denoted by $X \sim \text{Bern}(p)$.
\end{definition}

\begingroup\belowdisplayskip = -10pt
\begin{definition}[Indicator variable]
    Given some event $A\in\FFF$ the \textbf{indicator variable} of the event $A$ is given by,\[
    \II_A(\omega) := \begin{dcases}
        1 & \text{\textcolor{black}{if }} \omega \in A \\
        0 & \text{\textcolor{black}{if }} \omega \not\in A
    \end{dcases}
    \textcolor{black}{.}
    \]
\end{definition}
\endgroup

\begin{remark}
    $\II_A \sim \text{Bern}(\P(A))$.
\end{remark}

\subsection{Binomial distribution}
\begin{definition}[Binomial distribution]
    Consider a sequence of $n\in\NN$ iid Bernoulli trials with parameter $p$, count the number of successes and denote this by the random variable $X$ then $\text{Im}X = [0,n]$ and, \[p_X(x) = \binom{n}{x}p^x(1-p)^{n-x} \quad \text{\textcolor{black}{for }$x\in[0,n]$}\textcolor{black}{.}\]
We say $X$ follows a \textbf{binomial distribution} and this is denoted by $X \sim \text{Bin}(n,p)$.
\end{definition}

\subsection{Hypergeometric distribution}
As we have done previously, consider of urn of $N\in\NN$ balls with $K\in\NN$ of these being white and the remainder being black from which we will draw $n\in\NN$ balls and want to consider the DRV ($X$) for the number of white balls drawn. When drawing with replacement we have $X \sim \text{Bin}(n,K/N)$. However, when drawing without replacement X follows the hypergeometric distribution.

\begingroup\belowdisplayskip=0pt
\begin{definition}[Hypergeometric distribution]
    A DRV $X$ follows the \textbf{hypergeometric distribution} with three parameters $N\in\NN_0, K\in\NN,n\in[0,N]$ if $\text{Im}X = [0,\text{min}(n,K)]$ and, \[
        p_X(x) = \frac{\binom{K}{x}\binom{N-K}{n-x}}{\binom{N}{n}} \quad \text{\textcolor{black}{for }$x\in[0,K]$}\textcolor{black}{.}
    \]
\end{definition}
\endgroup

\begingroup\belowdisplayskip=-10pt
\begin{lemma}[Vandemonde's identity]
    \textbf{Vandemonde's identity} is an important tool in the derivation of the pmf for the hypergeometric distribution and so is included here. The identity is as follows, for $k,m,n \in \NN$ with $k\leq m+n$, we have:
     \[
        \binom{m+n}{k} = \sum_{i=0}^k\binom{m}{i}\binom{n}{k-i}\textcolor{black}{.}
     \]
\end{lemma}
\endgroup

\subsection{Discrete uniform distribution}
\begingroup\belowdisplayskip=-10pt
\begin{definition}[Discrete uniform distribution]
    A DRV $X$ follows the \textbf{discrete uniform distribution} over a nonempty set of numbers $C$, denoted $X \sim \text{DUnif}(C)$, if $\text{Im}X = C$ and,
    \[
        p_X(x)=
        \begin{dcases}
            \frac{1}{\text{card}(C)} & \text{\textcolor{black}{for }$x\in C$} \\
            \omit\hfil0\hfil & \text{\textcolor{black}{otherwise}}
        \end{dcases}\textcolor{black}{.}
    \]
\end{definition}
\endgroup

\subsection{Poisson distribution}
The poisson distribution is commonly used for modelling the number of events occurring in a certain time period. Its pdf is derived by taking the $\lim\limits_{n\rightarrow\infty}p_X(x)$ where $X \sim \text{Bin}(n,\frac{\lambda}{n})$ for some $\lambda \in \RR$.

\begingroup\belowdisplayskip=-10pt
\begin{definition}[Poisson distribution]
    A DRV $X$ follows the \textbf{poisson distribution} with parameter $\lambda > 0$, denoted $X \sim \text{Poi}(\lambda)$, if $\text{Im}X = \NN_0$ and, \[
        p_X(x) = \frac{\lambda^x}{x!}e^{-\lambda} \quad \text{\textcolor{black}{for }$x\in\NN_0$}\textcolor{black}{.}
    \]
\end{definition}
\endgroup

\subsection{Geometric distribution}
\begin{definition}[Geometric distribution]
    A DRV $X$ follows the \textbf{geometric distribution} with parameter $p \in (0,1)$, denoted $X \sim \text{Geom}(p)$, if $\text{Im}X = \NN$ and, \[
        p_X(x) = (1-p)^xp \quad \text{\textcolor{black}{for }$x\in\NN$}\textcolor{black}{.}
    \]
This can be seen as counting the number of Bernoulli trials with parameter $p$ that occur before a failure.
\end{definition}

\subsection{Negative binomial distribution}
\begingroup\belowdisplayskip=-20pt
\begin{definition}[Generalised binomial coefficient]
    Let $\alpha\in\CC$ and $k\in\NN$ and define the \textbf{generalised binomial coefficient} by,\[
    \binom{\alpha}{k} := \frac{\alpha(\alpha-1)\ldots(\alpha-k+1)}{k!}\textcolor{black}{.}
    \]
\end{definition}
\endgroup

\begin{lemma}
    For $x\in\NN_0$ and $\r\in\NN$ the following identity holds,
    \[
    \binom{x+r-1}{r-1} = (-1)^x\binom{-r}{x}\textcolor{black}{.}
    \]
The generalised binomial coefficient as well as this lemma are necessary to have a well defined and valid pdf for the negative binomial distribution.
\end{lemma}

\begin{definition}[Negative binomial distribution]
    A DRV $X$ follows the \textbf{negative binomial distribution} with parameters $r\in\NN$ and $p \in (0,1)$, denoted $X \sim \text{NBin}(r,p)$, if $\text{Im}X = \NN_0$ and, \[
        p_X(x) = \binom{x+r-1}{r-1}p^r(1-p)^x \quad \text{\textcolor{black}{for }$x\in\NN_0$}\textcolor{black}{.}
    \]
This is the distribution of the number of failed ii Bernoulli trials with parameter $p$ before $r$ successes have occurred.
\end{definition}

\section{Continuous random variables}
\section{Common CRVs}
\section{Transformations of random variables}
\section{Expectation of random variables}
\section{Multivariate random variables}
\section{Generating functions}
\section{Conditional distribution and expectation}

\end{document}